{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTtvKkeTyXq5",
    "outputId": "3ad5ea4d-354e-43dc-8748-ffc49f13ec4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets>=2.18.0 huggingface-hub>=0.21.2 fsspec>=2023.12.0 transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuMYtpQNy48Z",
    "outputId": "b81d835a-da3f-4f81-bfdb-b288a1e761b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers tokenizers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpjJ2rimUJRU",
    "outputId": "2235b263-db43-4f57-dfa2-7bfdefddaf85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers tokenizers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTUveOoqp0LG",
    "outputId": "5c8ee43e-951c-4f53-d369-f664a10e1f73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "4f289f4d913f4586a1ffd871bdf737bb",
      "e17af98c62d94593b03246cc90b6d3cb",
      "863417e5cc1e4770a604ff926b168d9e",
      "74dc28a671854d239003326042ff3bd4",
      "e6a2724d72a5485f97e0a6e7ad9fa894",
      "1adeae4aed3b455b9d8c3d64e6829ae1",
      "6125fb4634db4b2d9be18c04a674c4db",
      "172f8a05f3814d77bde450dc64aaeefc",
      "0fbb45a510154901b2e79f5e778e39c6",
      "f219b25785634f3d84f81eae11aed849",
      "7662aa4c164f4f5eb2a4a9f3b9337ec6",
      "e04247f97e574ab4857e129511261941",
      "7d8e061c2b834c49a157dab0e98d59ba",
      "30bc08965ef949c8b0c4e14ec9439a10",
      "6794eee8e59e4427a61ed5cbb68a49fb",
      "1b4fedcb962a4f7fad1266184d7d217b",
      "b9be8824bf064257a10b10e6fc3165d1",
      "271dd481d8bd47968bf9407b85af2ef0",
      "8ffe329bc6d74c829c8a51508872dbbb",
      "f0533f28ebeb48a4b73e933a3630366b",
      "fc27ae5fc7184eddbbff0abc52c84a4a",
      "0f4545b6792c4d23a0f60d96a745c22e",
      "b21777a2b8b9465c8de3e1da587af062",
      "552f07c5840a45ccb70f406dc1392115",
      "158afadbe7d243fbaf247f8808fe878d",
      "87db3790d3bc4657b6bd7e01600abcd7",
      "790a8c8bb70d4e9d930af070cbb1a183",
      "e47f04a5fd2245ff812fca6783e09f68",
      "9d564e74c7ff4ee896ef82ea7db77ebf",
      "1b6bd4dcddb14927ad16c9330f41da4a",
      "c05b4f71e2264988a0fe9106960d62a6",
      "3b62a635421344119228b9c0e7c7268d",
      "d0f26c9ca5c14ff4bd069d70eb346d89"
     ]
    },
    "id": "kQGCnazyBivA",
    "outputId": "b6b17e3b-e1e4-4b0b-cd58-6f9f19c38e22"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f289f4d913f4586a1ffd871bdf737bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04247f97e574ab4857e129511261941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21777a2b8b9465c8de3e1da587af062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from datasets import ClassLabel\n",
    "\n",
    "# Define the same label format used in yelp_review_full\n",
    "custom_ds = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/model_checkpoints2/sarcastic_reviews.csv\")  # This returns a DatasetDict\n",
    "class_label = ClassLabel(names=['1 star', '2 star', '3 stars', '4 stars', '5 stars'])\n",
    "\n",
    "# Map integer labels (0–4) to strings\n",
    "def convert_label(example):\n",
    "    example['label'] = class_label.int2str(example['label'])\n",
    "    return example\n",
    "\n",
    "# Apply the conversion\n",
    "custom_train = custom_ds['train'].map(convert_label)\n",
    "custom_train = custom_train.cast_column('label', class_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2af117f2de1a42edbbdfefc5c2da1b8e",
      "a1a8df16a6fa4cf1b3cdadee3f26b63c",
      "500c50a1f0ae445ba6f86f1400ddf692",
      "ca5ba23764924837a7300c2693463404",
      "e8f5112115ec48fc8a4affbb93275323",
      "c8f21ccaf10b4eefaf7819f2292fd7dd",
      "ab08fde73ed94f0084601fb23c3148a8",
      "a75cd436a330410f9a290b59ec5d9e49",
      "bde146f970144d31a504b6920b2f1e19",
      "36b8e6393fac4d12b722a038a74654be",
      "93c4b878c46c4f89a3b6c5fe4e7cf3ac",
      "1df427b0aa3e44628ee4b80e76b69b96",
      "c2d687deeb004d17bb9c6739671008a5",
      "d0357aa72a3f459d89d21675307e8d3a",
      "c98fdd17c905456d92ba469a6ac33d13",
      "5efe0726400a4ea49ac37d4fb7b06034",
      "8ebee19846594218a18119f9ed1ffbca",
      "d5a5de36a4f7460fa1fa1c94458fa40f",
      "83508902901448ca8264688eb3b8fff4",
      "3c5108f7a17749938d2ba6277814134a",
      "5504cb8092194a368679aee5bb00be35",
      "045d05b3c62546a787ed478979ac2b66",
      "3b3c597dda4347b9b8c420884a138762",
      "c5be80e2bf1b4b23a764a02efacbf60f",
      "2da085a9d1b74386b2ea0cdc185ebfb9",
      "d617650f4dc54b32b7784de6db39e7cc",
      "e2712bc4c51b485bbfa48485ecaffe46",
      "984642eb251c4467be2529ceeb22a6e6",
      "1e362dd1e6db4a95867c4963bf58e00f",
      "fd2e9d1be12e4c7d831b0cc0f6feba20",
      "0c28794c28b2493f8e78e71046186f0b",
      "d6416f13de8946298467cf4e3b5ae614",
      "b9f300e61b504aeb93872024d64cacb8",
      "599b6907a10c42209582f72eca054d6b",
      "d665145791d04f739be2747ce8541cbb",
      "2aab38f011ba4db9920f6bfc30abd490",
      "752a7a9cab3f4688a0ebe611b643b0e4",
      "a32be3f8976c424b94f4b45ceb8be7dd",
      "23891479f8b743d49a6e1801cac138a3",
      "be9b678080864eec80e5a4ccd3b63d8b",
      "71d5dcb1999a4358af97f3309e1709e3",
      "e1da33a0b9d34ee2bf940ef3ceb8caee",
      "f27c2a504faf48359f56e5622ac35540",
      "82001bb1a46940adba816d36d37ecd68",
      "646e3d5ee8384751914ab81622df6366",
      "157e50f426394136beb89a95f1cf2bee",
      "f7e4a0faf5314391946d296837a2eeb9",
      "db434d0c849b485f93de452c0cb28510",
      "4b1ef2dc51ba4cb591da33abd30cab71",
      "a584eb37995a4b6cb0b9844a6ac6777b",
      "3e80de4ccf3b49198432517bda5d8aff",
      "bd3eb0a607e14586ab73498b75577023",
      "6cf112df871a4c9298ec8c585ae7b3ec",
      "6c41d02225734b3bbc0907e59bf9d256",
      "7f1b144e03664e5d83b93023899c75b2",
      "8eef40b12dd74b1fa302f7997ff0ed8a",
      "08475a9f7ead493f91137e19d0eaaf4b",
      "ace207339b5b4afab6d9d706a0f6846e",
      "cddbe07045324981a2baf98452232184",
      "3e29361727a9463a835f4cf8c7bd7fac",
      "d864977b60b04824b49ea4a87ca8a5a7",
      "738e34ab7b29452ea761797f458fbeb9",
      "6fc9099205bc4cc6ba8324f24d8420ab",
      "dbf53a75d40c4c51be49803b0584e9ad",
      "e27fc94d84aa4c03bd967e7896d05abd",
      "091c6c1c85fc49a5bd293b59330378a8",
      "d3553f79696044709a06844a5ab926b7",
      "bc6e6a64316b4a23989fae19003ea491",
      "d8d1b3039c6249fd9ab02f734c750fba",
      "1f2ac62a603044f588a33712216d8ca7",
      "0d7206e26fec4f5e8c20621163bd586e",
      "ba1b512cdea64b9687f7dba0bebca88f",
      "b6e4d693137c453e84298f0b4530dd0b",
      "8990f1e3117b43ad879065823d683e5e",
      "008b9d38562f4abd8aa45c9989a970e8",
      "5c8c5ed29803400181cc7da7cb324a46",
      "08e547a5cc644127af8fe034760c5673",
      "d3b0534543174f819efd31a5f4d9b38e",
      "e7789b59b94b44d6bc43492dc4ee08f4",
      "ed20c6d6dca54c30904547884a0ef8c2",
      "6e4bbc59c3cb4157aed2a5ed208d955a",
      "b795892ee32144faa756c531d8b8ad1b",
      "178c2d7cab834f6292cd5f93fceafbff",
      "061731c2617749bd9a07337f86e80a72",
      "0f614d9b6eb640f3a5d21701d7286f2a",
      "9e7b5f08333649daac59f9548a87cad4",
      "9eb2dc959fa9454db8f3d34d22b75a0c",
      "90c2a3ea955d47ad9acacf96e1ea4837",
      "28ea15915ad24d3dbf066f613828db8d",
      "9d3d9c5de5cf492e99cd2314cfc57d67",
      "19ffb1dff4534609b9096e776bcf5278",
      "0077227102b644f8bd944aa52e384797",
      "9de15e666c4346379f222e3b9191162f",
      "3989b562144c44209e1eb4046e0af788",
      "bf1b6e578c1d4072950b24b514ad7e2c",
      "b9b32664b5e24512b64b732807b360bf",
      "ad64ef7f0766465d859b4fe9d2077261",
      "4a5220c5f91240c49f97bc0996575014",
      "a154411219d74064b06bfaeef79300ae",
      "d3dd08da512e4437a8ba0892eefa1b8c",
      "82edbedbaba34165b57fc3bcf42ab812",
      "3994fce6a02c4c338197020b86a43aa5",
      "2031dbfa7a87414f8460f0dfeb13224b",
      "bcead7827d774d14973f1b99d0938cdb",
      "6d124406864b408abec4bcfc10dd118a",
      "5af9a03a4f5c40529fd926189c87cfb9",
      "df2223bce10b48158e0a4a86e1c2214e",
      "001ce27b92e24cd4bd31ace793acd848",
      "52d9f40ce37e403aa88f93d7786b7d81",
      "28cdeacd5fee4efba1944c8428cfa970",
      "55e0f29196c144139d4dc6672d39064b",
      "2489b059a96f41e1b46c153f157f2ee4",
      "983edff81c7e4ed796f3d5307419d5c5",
      "68b874c837944fa295eea695117a4f22",
      "77a6dc09e82c408a998b13368c1b1992",
      "19c0e5be828a41d0ae50e9ceda7eef8e",
      "ff7f68201f45429ea5aceacb49782840",
      "d93c13dcf33b4a3095ed94a07d9a83ff",
      "a9d979cdcb9c4bc9b7ad4c0c3350f417",
      "75875423341440ce8a80e8dacc211462",
      "9f7a26964568404f971f3b1836c78745",
      "7e739bd38aaa4f5b87f41c7eaab68d23",
      "01688cefcbf84e23b26b5f0fc5903211",
      "3923ad688ae941b896fa6026c6782d89",
      "6cfd13d47b31451e8549c86cee5b4e21",
      "61d5f26c6e124bfe8cbb897a2c8ef14f",
      "f44df71c116e4fb58a4ff883c979d2a6",
      "801cef62f75b4e5fbd9190db46588e24",
      "1e903d53cf1449908ae93aceaf547144",
      "faad83b568964b1d9190b79133476529",
      "dec11a38f9144e20a029eab15701cc77",
      "a4ec85381bdd40aa80a50bb4ab6dcd55",
      "e9326711520f4087907de362b4a12ad7",
      "c18774458c30439ab949fcf0316b869a",
      "e95f92806afc4368a949eaafe095c981",
      "7d46af0364f040f98811f668dae2bf88",
      "c6ff86009de94043a9f10356a5888d4f",
      "186e4dad81fc45299cc6a9113bd2c736",
      "e2cb664422624e0e9f02380309716e0e",
      "8c8903eb176c4e8b8188066666cd0c2d",
      "3230c09823ce4017863da240aebf3954",
      "c97f211190db47908c7b20c778d713dc",
      "51a8d651af2f4834971d5f4ad629b06f",
      "e891fb03e25843a0845e8cd3a49a66ee",
      "0d25aa7ee065446784015a1ba40db6f0",
      "bd25f802d7524874846a1ee9cf77fef5",
      "db171ceaec694b38bd264e0bdf8e96d3",
      "6f07f995bdfd4cd4a0de594410a8c042",
      "256aa00b04514cd4aa3f93c82756ab5c",
      "c96dfd73148546438fd2df441d0d1db0",
      "bab43651f5c44cb5974b758f663a30b0",
      "eebc080c3d3c461093ef1e416af1bf74",
      "1da623763ad744799d924032e47540f6",
      "ad61e68e361e422d992f1042b4f3f263",
      "fdd7bc0029e047bfbe9bf610fb426542",
      "138d36a0a17c4b1eb669b3d24ab3fe4a",
      "837b622ed66a4a929847790ea100cac7",
      "643f559ac56c427293db7f6931abcb31",
      "1514e0ce16b94798853efca155f80dfc",
      "084527d190a2447b89c6336af440f29a",
      "35a5fc3ec29f4fbcbb2cb08deaca70c2",
      "13c1835d71024fd4b1395bdd17ffe27b",
      "0601b74d466947faa52adfab31457532",
      "92c1d482f0ae401780f8e487ac01cad5",
      "be283522d4da4b589416608fe9ad51ea",
      "a464bbf8747844c68a1db4b74df48f11",
      "d8347d90c97b4684bc351282756829e1",
      "b0ec3af0c89f4c9e83ee56a8e9cb8da8",
      "a97f81fb0485459cacdbc058e357453d",
      "55cbd34f921a4bba9777f1a5dba5ec19",
      "bf9c9f21fb584237bad2e66eab13039b",
      "06b10682234a41aba1d5516b510e0253",
      "8fe9cc6418e748fba1672d00b47a5ed7",
      "928425056639471ca63090ae7d5f394a",
      "06d1f53ac95148cebfc1c568ed9e6abb",
      "91417b5c74a747b7b62f072309268181",
      "ea8d7fa0bbeb4c4c93734505cf1070e3",
      "6e641b93940b424a9e35ce10beb4acf8",
      "cfcd5181babb4718a8572f05806db741",
      "7c8c615450054ff3ae6d7537b47d0a8b",
      "250a05c24e7d4057b1005169cf4b177d",
      "e8fdd5d5569c4a15abcef90984f5378c",
      "9d759d5f8c45400a97e5824256367aaf",
      "260ebf2fd53c460a87ae252a7e59e279",
      "685d6b5f24944f58bd84290796cb9bd0",
      "e78a3bafc1ae481f932d1a9a41e3d96b",
      "42ba759f39244fbfb0e3f5406db3cb74",
      "0864daab9155415096bb926acb87915e",
      "c7e2dbf1fbc8494dae9c36bce4877869",
      "7d5946591b46418f915d8b9b53a1aafc",
      "29fad5d9bcff483dbfc713daf46c9913",
      "b572e54fbf6a43799ed05534a72fc193",
      "62759f56c48d4290a7a4ad6c39152a13",
      "b7bc29d79b5442f5a5a93e42a1844071",
      "bd2cbd2e24904d70ad2c4970d7a97063",
      "ff46b223e8824ee2b0b17b3e34dc41c5",
      "0e5c1ba5f6cb4ed4b89e8ba79ff2e514",
      "69823e44147245c5a61f4b0d3b5bd645",
      "f5e75fffc84b4740be9cccb740e887aa",
      "5f2babd444d34cf9be4ea38e733f5a84",
      "18cb3cde10f148c0ae55f4f6aa1af0fa",
      "dd9f1d7f1eed4f479f1bda86a9309ce3",
      "b6688fc882034b2dbb269970dae472bc",
      "7a7c8566b73840a289a1c3aa717155ca",
      "4f701262ca0a467c90438b59f5d2299d",
      "1f80117efc404a5691e43e83da5917df",
      "43349f6bed9740f99aa37e8404f6d9e8",
      "129ac960851047869eddde3401905d23",
      "0783554b71334887b568f65b78ff3a70",
      "bae97d7468a745e289ea750e2d5ad98c",
      "071a05b49bb54a9b80dfc44ef03156d9",
      "6fb683fa141a489ba8db17c31a42692b",
      "d783750adb9e4c5da11321cc74209797",
      "d34a144b1ccf421a8aff57a7a5952cc2",
      "5f271070e5c6443ca19a69d0ab19d0a8",
      "3f12dd1e547c4281a35df52d1930a5ee",
      "42a9fa1f8ebc4004a3519e16dda436cd",
      "4b08b546a972403baeccbf4df0a1a70e",
      "c4c7ef0f1bde473ba3c5b5f3b6065e00",
      "40a8d9f2d399476eb2f13ac68eb60fb9",
      "dff1a81ffb8741a18006aa3653528a54",
      "a33e1d66cc5742859e02a658d40d45f4",
      "e2ae86fb14bb46bca6f12ba6ee802e7c",
      "4a359d314edf493da4c0dcb8ad27b77b",
      "5a45b87c1c6f489fa88067724076cccf",
      "7532c66e08ba43b3b68a2e692bd20b80",
      "1e2c27d1a74c48a8906716dc9ad35d9d",
      "04c1bf31abd6448482ee21b83011e6ad",
      "8f3a59a311174ba585fc3a16296e8003",
      "03a8da288d9c46afbf81c98488e3d89a",
      "ef75814a6af8454f8f7be876584ed644",
      "3f2e542d10c64953942b0b59043e782f",
      "71cd98c597674b92b1f0f2e286fbc8b1",
      "21bbc74d21c64a9c9ffbb7e1415d78b5",
      "e495a51ac37844519f40cd1fa68ae46a",
      "57db0f78b0da479ea9be92d5850e2305",
      "4fe02b6d436a467aa6c310d2f75a4622",
      "51fbbdb57a8c4600bec1bc8fa69231fe",
      "baca8c210b364e01aa58b36f5d174be1",
      "e75993c574664aa78c9dde12cf00652c",
      "db62916ac3e74e7a83c6ede159dd06e8",
      "13bf516a46d245d09e5209016eb3666b",
      "e82a80fef4144e05ba44053910f31d0a",
      "edf2f97c31ce472391fd5dca297bed93",
      "77994805409a4fd9a12f89f7a3bf2700",
      "ec9d43c7bf8342e6abcc9f2245a6310e",
      "a00f03bfd26b4b2bbaa65d2de20ca751",
      "aad019c0cb634169bf2fd17a0e748766",
      "b7604ea9c8a44f0d9ab26e12ac5b3876",
      "0475fcb0a5bd4528b3e89ca80b44c5ea",
      "896e9e9335424d0d8d55044aea97265b",
      "3422c746268e48d99de30adb83de39ee",
      "7e835b656fef404f9032c4b49b163cc7",
      "eea280e0de154e79bb6279d7afc86d23",
      "e471b0cb0f004421ab91e7cb6c9acedb",
      "b4feb07da18c480d8f1dafbd7c1e8fab",
      "80cf6315adc541988ad930914176ed9e",
      "4239de027da545118e31dfa981745c3a",
      "0a2b7dd202b0492096cdc55565feef6b",
      "b2dde91421474bd3916e07b04c6d9e01",
      "8a254a44606c463381f5859dcedc8525",
      "a6e0726f2bca42d28540aca609333fb1",
      "27ffb43bd1da4e6f9931e3f042a72606",
      "d75c65b94aef47598da1a7ced121924a",
      "07af8984ed9a4d0783734aad920eb0b2",
      "d95796375738493dba1bd69740dbbb7c",
      "3715db29f79a44cfbef1cb9064454eb9",
      "015c0128e24a47a7a56c2fa87174e517",
      "d2601a759ffd4c889b41f41056d030c7",
      "31cb3f55ec4c4fc69941509a4eb955a2",
      "b419f2a340764826af43677d6ada1ebe",
      "0e22b6e6444a48a8a4bbd199c6a0e99e",
      "234d0f4c01f14aed96589db73cc06f52",
      "865cab0813734858a832ac138cb9a52e",
      "b3bebd4f89954bd08407eae30e19e4a6",
      "0d117dd386ed4510931e752ebcb62495",
      "91b573e8677040bfa551da94d25f55cf",
      "730731130c1245139152e38a70ae61ac",
      "02609cbf33d7432aa8aba7a7e5e43b70",
      "f0c9f464edff40fe8251ede156d4b302",
      "1628511c3bc44c0992867d9a149be1c5",
      "4c4bec43fa3141579c374d7f26a1ffc0",
      "5d68d31be5f24e3a9787261c1a27d31a",
      "0ae9e8cd344545e9a9d79f4a722b977d",
      "7d5185daeb904466ad993307d045342a",
      "a8d08b330d7547cd9d3866c2a3d26582",
      "9591a8416abb4c289df41bed2cf36a1f",
      "36471a4c2cea40ec96e8e2a0a8a84e3b",
      "ca4906d8086044f7979565c7c56e0474",
      "98b09c8b510c4ad3b3e0db54df598481",
      "4aced40be6f94af2a4697ba09e1ee5ff",
      "b8c580c1a3114b318bd81f5a18557a85",
      "9d2345eecc8a4d6ca968dd7aff371b43",
      "982cd1eda85b4f50abefd3524f95a122",
      "91e47c8332a348fe88e9305483a115a6",
      "b636ad3ef16a47ab9730566bb931b5a4",
      "663182e01f7a42c8ad7e9b41cbf5ee86",
      "13aefc0727024b678d98ff415b726d04",
      "1e03069102d1450f9984482f8ce9754f",
      "dd93d029f40c477dab5a159685ff24cc",
      "7f0659446e164da78b1724505a44b635",
      "aebec51060aa429880482df1509f84a1",
      "d0e36bc444cd45cc89f45a0cf8cb936e",
      "6b86b7de3d75433f8bdda72b615ffa26",
      "4d6228e139db4076916f7c2b646fac41",
      "50f92a28a473467086a01dac917c0a42",
      "ca72026d5c994a17827af6642a382df8",
      "fae5624d30d245e78f2d33d34a22b98b",
      "fcd37a1e9d1a47e9b411275720d28ea7",
      "8784643b3dd4432496b6f95df0451cfd",
      "b4dc9d3dcccf452a8405528187bf1bfe",
      "288983df24cc4baab31483822863d69d",
      "3f3cea8409f540aa9953aa6bddd74524",
      "f76531ea79344a78a600c1cfd8998ca7",
      "7755dd8120dd4ecf82e67cb58c196b1e",
      "dda48b11a2f74669af75dbfcfb4c0ee0",
      "8c0dfb6b3eb741cebe23e785329985ba",
      "2301d9f0a9c64a608e11c0d8e956b38a",
      "fd64e2ab19a04e3da2ea3906fca9af12",
      "a52946a0e8a44bd59179048354f3b7d0",
      "d3da70cabe924636be3ac7a44e6fb5ee",
      "152fc2487d7b4262bc6f4dd78bcb07d1",
      "975d5c33e2b74d84b51160ddaf340fa8",
      "04329e4d60b64ed1bb574a8cf6569356",
      "a82da49d753d40d2a45fa7a69ebac0fe",
      "ebdde0b835de44afbff1fb596e14a34e",
      "c8ab686269f54617bdc3f35c2a40f02c",
      "038a3150aca749d8b25eaab09d59475b",
      "2065d47e59b14655aebd1f2fb74367ec",
      "43b89ec0c96e4e59bf95d1f7e6f0ff6b",
      "734fd70c06724e3f93b6079cfb36c66f",
      "e1856fe863084070b685b44d1e4194b4",
      "d9610d4a5f134757bb16133152e48474",
      "c64f18755bdb4dc99dccc233cb83918b",
      "7ea680caf63842bb82c52cd0c64ad22b",
      "9759d50a1c3d40e1ab32e928dc322ed2",
      "2c18fab9ad074f5cb083d379e1639c61",
      "8d60f0d1d4734884ad7a877417e244d1",
      "f5feea1216114ffeab0427e65372fb33",
      "910c924ecfce46c98b09f9251050b984",
      "b7b1e4a6af434af3b18df969b3b95437"
     ]
    },
    "id": "W83WjJlAfsCM",
    "outputId": "c30ba09c-7db6-45ad-9f0f-67aad70b7309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'yelp_review_full'...\n",
      "Detected 5 classes in dataset 'yelp_review_full'\n",
      "Training BPE tokenizer...\n",
      "Tokenizer vocabulary size: 30000\n",
      "Creating memory-efficient datasets...\n",
      "Training samples: 650436\n",
      "Test samples: 50000\n",
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af117f2de1a42edbbdfefc5c2da1b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df427b0aa3e44628ee4b80e76b69b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Time: 743.26s\n",
      "Train Loss: 0.7662 | Train Acc: 0.6710\n",
      "Val Loss: 0.8769 | Val Acc: 0.6202\n",
      "Checkpoint saved at epoch 1\n",
      "New best model saved with accuracy: 0.6202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3c597dda4347b9b8c420884a138762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599b6907a10c42209582f72eca054d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Time: 861.25s\n",
      "Train Loss: 0.7583 | Train Acc: 0.6745\n",
      "Val Loss: 0.8873 | Val Acc: 0.6196\n",
      "Checkpoint saved at epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646e3d5ee8384751914ab81622df6366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eef40b12dd74b1fa302f7997ff0ed8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Time: 867.66s\n",
      "Train Loss: 0.7512 | Train Acc: 0.6782\n",
      "Val Loss: 0.9086 | Val Acc: 0.6202\n",
      "Checkpoint saved at epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3553f79696044709a06844a5ab926b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b0534543174f819efd31a5f4d9b38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Time: 886.97s\n",
      "Train Loss: 0.7439 | Train Acc: 0.6813\n",
      "Val Loss: 0.8885 | Val Acc: 0.6202\n",
      "Checkpoint saved at epoch 4\n",
      "New best model saved with accuracy: 0.6202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ea15915ad24d3dbf066f613828db8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3dd08da512e4437a8ba0892eefa1b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Time: 875.97s\n",
      "Train Loss: 0.7381 | Train Acc: 0.6846\n",
      "Val Loss: 0.9069 | Val Acc: 0.6205\n",
      "Checkpoint saved at epoch 5\n",
      "New best model saved with accuracy: 0.6205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e0f29196c144139d4dc6672d39064b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e739bd38aaa4f5b87f41c7eaab68d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Time: 915.43s\n",
      "Train Loss: 0.7301 | Train Acc: 0.6886\n",
      "Val Loss: 0.9081 | Val Acc: 0.6206\n",
      "Checkpoint saved at epoch 6\n",
      "New best model saved with accuracy: 0.6206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9326711520f4087907de362b4a12ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e891fb03e25843a0845e8cd3a49a66ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Time: 875.10s\n",
      "Train Loss: 0.7242 | Train Acc: 0.6911\n",
      "Val Loss: 0.9152 | Val Acc: 0.6208\n",
      "Checkpoint saved at epoch 7\n",
      "New best model saved with accuracy: 0.6208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd7bc0029e047bfbe9bf610fb426542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a464bbf8747844c68a1db4b74df48f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Time: 890.47s\n",
      "Train Loss: 0.7162 | Train Acc: 0.6945\n",
      "Val Loss: 0.9229 | Val Acc: 0.6176\n",
      "Checkpoint saved at epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8d7fa0bbeb4c4c93734505cf1070e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0864daab9155415096bb926acb87915e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Time: 870.69s\n",
      "Train Loss: 0.7107 | Train Acc: 0.6978\n",
      "Val Loss: 0.9193 | Val Acc: 0.6200\n",
      "Checkpoint saved at epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e75fffc84b4740be9cccb740e887aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae97d7468a745e289ea750e2d5ad98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Time: 901.02s\n",
      "Train Loss: 0.7033 | Train Acc: 0.7011\n",
      "Val Loss: 0.9415 | Val Acc: 0.6199\n",
      "Checkpoint saved at epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff1a81ffb8741a18006aa3653528a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2e542d10c64953942b0b59043e782f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Time: 837.74s\n",
      "Train Loss: 0.6970 | Train Acc: 0.7038\n",
      "Val Loss: 0.9396 | Val Acc: 0.6172\n",
      "Checkpoint saved at epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82a80fef4144e05ba44053910f31d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea280e0de154e79bb6279d7afc86d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Time: 894.07s\n",
      "Train Loss: 0.6912 | Train Acc: 0.7062\n",
      "Val Loss: 0.9762 | Val Acc: 0.6188\n",
      "Checkpoint saved at epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07af8984ed9a4d0783734aad920eb0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d117dd386ed4510931e752ebcb62495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Time: 836.24s\n",
      "Train Loss: 0.6825 | Train Acc: 0.7104\n",
      "Val Loss: 0.9728 | Val Acc: 0.6165\n",
      "Checkpoint saved at epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9591a8416abb4c289df41bed2cf36a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13aefc0727024b678d98ff415b726d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Time: 837.58s\n",
      "Train Loss: 0.6776 | Train Acc: 0.7129\n",
      "Val Loss: 0.9823 | Val Acc: 0.6144\n",
      "Checkpoint saved at epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd37a1e9d1a47e9b411275720d28ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52946a0e8a44bd59179048354f3b7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Time: 853.83s\n",
      "Train Loss: 0.6712 | Train Acc: 0.7154\n",
      "Val Loss: 0.9826 | Val Acc: 0.6164\n",
      "Checkpoint saved at epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734fd70c06724e3f93b6079cfb36c66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Transformer Text Classifier with Hugging Face BPE Subtokenizer\n",
    "# Import required libraries\n",
    "\n",
    "# Cell 1: Install required packages if not already installed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import math\n",
    "from datasets import concatenate_datasets\n",
    "from datasets import load_dataset\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# Cell 2: Initialize and train BPE tokenizer\n",
    "VOCAB_SIZE = 30000\n",
    "MIN_FREQUENCY = 2\n",
    "SPECIAL_TOKENS = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
    "\n",
    "# Train a Byte-Level BPE tokenizer on the training texts\n",
    "def train_bpe_tokenizer(text_iterable, vocab_size=VOCAB_SIZE, min_frequency=MIN_FREQUENCY):\n",
    "    # Use a small sample for training the tokenizer to save memory\n",
    "    if isinstance(text_iterable, list):\n",
    "        sample_size = min(30000, len(text_iterable))\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        sample_texts = random.sample(text_iterable, sample_size)\n",
    "    else:\n",
    "        # For dataset iterables\n",
    "        sample_texts = []\n",
    "        for i, text in enumerate(text_iterable):\n",
    "            if i >= 30000:\n",
    "                break\n",
    "            sample_texts.append(text)\n",
    "\n",
    "    tokenizer = ByteLevelBPETokenizer(\n",
    "        lowercase=True,\n",
    "        add_prefix_space=True\n",
    "    )\n",
    "    tokenizer.train_from_iterator(\n",
    "        sample_texts,\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=SPECIAL_TOKENS\n",
    "    )\n",
    "    tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\"))\n",
    "    )\n",
    "    tokenizer.enable_truncation(max_length=256)\n",
    "    tokenizer.enable_padding(\n",
    "        length=256,\n",
    "        pad_id=tokenizer.token_to_id(\"[PAD]\"),\n",
    "        pad_token=\"[PAD]\"\n",
    "    )\n",
    "    return tokenizer\n",
    "\n",
    "# Cell 3: Transformer model architecture (unchanged)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TransformerInputLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.positional_encoding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):     # apple [128] ---> Wq , Wk , Wv  --> Q,K,V  = Q.K = attention(0.78) --> [128] -->king\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, D = x.size()\n",
    "        Q = self.q_linear(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.k_linear(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.v_linear(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(self.dropout(attn), V)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.out_linear(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_ff//2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(d_ff//2, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear3(self.dropout(self.relu2(self.linear2(self.relu1(self.linear1(x))))))  #--2 layer\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attn(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        return self.norm2(x + self.dropout(ff_out))\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout):\n",
    "        super().__init__()\n",
    "        self.input_layer = TransformerInputLayer(vocab_size, d_model, max_len, dropout)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, mask=None):\n",
    "        x = self.input_layer(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, d_model, num_heads, d_ff, num_layers, max_len, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, mask=None):\n",
    "        x = self.encoder(input_ids, mask)\n",
    "        cls_out = x[:, 0, :]\n",
    "        return self.classifier(cls_out)\n",
    "\n",
    "# Cell 4: Memory-efficient dataset class\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_len=256, text_field='text', label_field='label'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.text_field = text_field\n",
    "        self.label_field = label_field\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item[self.text_field]\n",
    "        label = item[self.label_field]\n",
    "\n",
    "        encoding = self.tokenizer.encode(text)\n",
    "        input_ids = torch.tensor(encoding.ids, dtype=torch.long)\n",
    "\n",
    "        return input_ids, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Cell 5: Hyperparameters and data loading\n",
    "D_MODEL = 128  # 512\n",
    "NUM_HEADS = 4  # 8\n",
    "D_FF = 512     # 2048\n",
    "NUM_LAYERS = 4\n",
    "MAX_LEN = 256\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 64\n",
    "LR = 3e-4\n",
    "EPOCHS = 30\n",
    "DATASET_NAME = 'yelp_review_full'\n",
    "\n",
    "# Cell 6: Load and process data\n",
    "def load_and_process_data(dataset_name=DATASET_NAME):\n",
    "    print(f\"Loading dataset '{dataset_name}'...\")\n",
    "    ds = load_dataset(dataset_name)\n",
    "    train = concatenate_datasets([ds['train'],custom_train])\n",
    "    test = ds['test']\n",
    "\n",
    "    unique_labels = set(train['label'])\n",
    "    num_classes = len(unique_labels)\n",
    "    print(f\"Detected {num_classes} classes in dataset '{dataset_name}'\")\n",
    "\n",
    "    print(\"Training BPE tokenizer...\")\n",
    "    tokenizer = train_bpe_tokenizer(train['text'], vocab_size=VOCAB_SIZE)\n",
    "    print(f\"Tokenizer vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "    # Create memory-efficient datasets\n",
    "    print(\"Creating memory-efficient datasets...\")\n",
    "    train_dataset = TokenizedDataset(train, tokenizer)\n",
    "    test_dataset = TokenizedDataset(test, tokenizer)\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "    return tokenizer, train_dataset, test_dataset, num_classes\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "def init_model(vocab_size, num_classes):\n",
    "    model = TransformerClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        num_classes=num_classes,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        d_ff=D_FF,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        max_len=MAX_LEN,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    return model, criterion, optimizer, device\n",
    "\n",
    "# Cell 7: Checkpoint functions\n",
    "def setup_checkpointing():\n",
    "    if os.path.exists('/content/drive/MyDrive/model_checkpoints2'):\n",
    "        CHECKPOINT_DIR = '/content/drive/MyDrive/model_checkpoints2'\n",
    "    else:\n",
    "        CHECKPOINT_DIR = './model_checkpoints'\n",
    "\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'transformer_classifier_checkpoint_best_best.pth')\n",
    "    return CHECKPOINT_PATH\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, path, device):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Cell 8: Training and evaluation functions\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"acc\": f\"{correct/total:.4f}\"\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Cell 9: Main training loop\n",
    "def train_model(model, train_dataset, test_dataset, criterion, optimizer, device, checkpoint_path, epochs=EPOCHS):\n",
    "    # Set up data loaders with appropriate batch size and num_workers\n",
    "    NWORKERS = 12\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NWORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NWORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Try to load checkpoint\n",
    "    start_epoch = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train for one epoch\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "        # Evaluate\n",
    "        val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        # Report metrics\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Time: {epoch_time:.2f}s\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_path)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            best_model_path = checkpoint_path\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "        # Force garbage collection to free memory\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Cell 10: Example inference\n",
    "def predict_sentiment(text, model, tokenizer, device, num_classes):\n",
    "    enc = tokenizer.encode(text)\n",
    "    input_ids = torch.tensor([enc.ids]).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        # Map score to sentiment (for Yelp dataset)\n",
    "        if num_classes == 5:  # Yelp dataset\n",
    "            sentiment_map = {\n",
    "                0: \"Very Negative (1 star)\",\n",
    "                1: \"Negative (2 stars)\",\n",
    "                2: \"Neutral (3 stars)\",\n",
    "                3: \"Positive (4 stars)\",\n",
    "                4: \"Very Positive (5 stars)\"\n",
    "            }\n",
    "            return sentiment_map.get(pred, f\"Class {pred}\"), probs[0][pred].item()\n",
    "        else:\n",
    "            return pred, probs[0][pred].item()\n",
    "\n",
    "# Cell 11: Save model artifacts\n",
    "def save_artifacts(model, tokenizer, device):\n",
    "    # Save model state\n",
    "    torch.save(model.state_dict(), \"transformer_classifier.pth\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    with open(\"tokenizer.json\", \"w\") as f:\n",
    "        f.write(tokenizer.to_str())\n",
    "\n",
    "    print(\"Model and tokenizer saved successfully.\")\n",
    "\n",
    "# Cell 12: Main execution\n",
    "def main():\n",
    "    # Load and process data\n",
    "    tokenizer, train_dataset, test_dataset, num_classes = load_and_process_data()\n",
    "\n",
    "    # Initialize model and related components\n",
    "    model, criterion, optimizer, device = init_model(tokenizer.get_vocab_size(), num_classes)\n",
    "\n",
    "    # Setup checkpointing\n",
    "    checkpoint_path = setup_checkpointing()\n",
    "\n",
    "    with open(\"tokenizer.json\", \"w\") as f:\n",
    "        f.write(tokenizer.to_str())\n",
    "    # Train model\n",
    "    model = train_model(model, train_dataset, test_dataset, criterion, optimizer, device, checkpoint_path)\n",
    "\n",
    "    # Save artifacts\n",
    "    save_artifacts(model, tokenizer, device)\n",
    "\n",
    "    # Example inference\n",
    "    sample_text = \"The food was delicious and the service was excellent!\"\n",
    "    sentiment, confidence = predict_sentiment(sample_text, model, tokenizer, device, num_classes)\n",
    "    print(f\"Sample text: '{sample_text}'\")\n",
    "    print(f\"Predicted sentiment: {sentiment} (confidence: {confidence:.4f})\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714,
     "referenced_widgets": [
      "fde84cdc05f94189bc29f389306abac8",
      "fefe67b0bfd04ab7a6f820f51d54b9ee",
      "bb56618b0ede4cc6b82d110551943a87",
      "a5579f24dc5f456abc17b38b52438ef0",
      "773a4fab02e44858b2a06d32da42de63",
      "bf8f4e9cbc3e4e57b393553d1a8cd6b0",
      "ba998b2490d74576be084c77f7f8676b",
      "6d47b36be0d14833a8c263e3c710ca02",
      "74b9bb15b0f34f22a42278be994d2665",
      "65c3050ae7b1416eabe42d497e29efc8",
      "db59404faace448798f8d0346ca2a8ec",
      "6b92d008b4604d0f8afc54a2acdcf970",
      "8521424135a34cf58529bb1885f06138",
      "b91d5d1dcc0946529ffb6a2e08a9e741",
      "7db55d9f8d464aaea935fd7c5fbfbcba",
      "b42c13b05b5f4f8b9abe475b2ac2cfa7",
      "60f50fe109914fe8864de1ec79d4b8c1",
      "120ac120e43d4985908bb588809dd3a1",
      "28c9adaa972547ff85dc51a9ba090d85",
      "32086b17c0fb4e84b54e595be7cb7926",
      "1605e0b7ada64b6e8c6ec0310ba24921",
      "9c9cc12fe3de4723ab8103320d06280e",
      "493fb77e42154b97b06a39281ffbce02",
      "c774ea6c6aee46339b834062eb12d5f3",
      "5a4e9fc354ef4fd397d11b67bdb10bb9",
      "fe98f40ad3c84b1d883b02b50cefe693",
      "1d93cb0a1deb4f189ea3407226bea0c4",
      "057f298b59454919b1ee2ab3214c0cd8",
      "c80eb601703c41c8a3a2da2cdb4254b4",
      "984c8519bb7740d19bb20f19168fb5de",
      "ea9793d7d2c948a1b11d9de77212ba29",
      "c6e6675cdfe44b788922f197de6afc75",
      "12f6363e33ef4247bce37d495f920e96",
      "dff00c7b2d474cea993f2ec74ee1b37b",
      "96ede2b472054c90a166f9bb669cec18",
      "30be3e5e9b3543fb8a4b466a3ac7ed54",
      "f0bd36e435f24dfd8e878d47ecf65770",
      "99cf0fcdf5b5420087bcfb0bbbd03204",
      "095281ad658441a5b86a0a60f50ac3c1",
      "ecbc8708ccb84153bd6f75faea3572fd",
      "299e8874c1b5418aa496f2adcac96a89",
      "864c696c7ca847e3b80221cdf024260c",
      "741d13bdd8a84c24a069fff58c15cf96",
      "3d3a6020c05c4b86b5df907fae80e451",
      "738b91cfe4ab446d908cd144248e7828",
      "d325937325764649aaa871434d654c6d",
      "3112e2b8edf743ec9d68fdd7f865946d",
      "6ac40e5a51a741f6897c3805956494fd",
      "7bdafd835a044501ae153c7e523b9af6",
      "edb78eb2ab194be9b4c32d3843f549eb",
      "a0f9cb71f47e4999b80ecda7c4ab29f7",
      "442611401e84438f948679c4175f67e5",
      "f9891ff8e13d41d9b5b5b9615bebc924",
      "0d142a54c2554ae0b64c251250253b8b",
      "065793ef3a6c4ce5b30eae60b6065021",
      "caceb86ca81c42aba7949ad737bd8e01",
      "cbfe373e56ec45a2a5a346ae2c2f820a",
      "e8735689ef9f4078bd60f213d8ff8168",
      "22bfce5094c642a98aca8d024e8cd7b5",
      "b55a87d5a16c4ff0b4b400174e13dadb",
      "699e52636cc54f208c8b8bb1c80e4665",
      "da9202bb91c340ea8a3b8dddcd6e1093",
      "6620d2a089474504bdd428b4568c1613",
      "8b37692235fa4ec7b43fac6524699775",
      "8b58eb35f3bb4b408a014dd7250972d7",
      "d4b191b0e1404dcf98e91683bb0d904a",
      "4494c1ed96ef4ae3b737c35788fdec36",
      "63af0f99ce0f4bf4a1bd38b252fdd1cd",
      "97d093ead7384e32a7bd141f1ecd878a",
      "ef2358bc11af439b93c0759b257899af",
      "794cc870a12b41f1b2d4bda730469840",
      "35dd327ef8a3453e892667026a9f32ad",
      "e269813645b14a2fbba470cec61832e4",
      "4eae45f055714725aa48fec9ef7fd80e",
      "92bbbe596e664a69a964a337aabaa042",
      "1ae7c45044724c6f8f4b9ca17d176b36",
      "d9a8d1da0104432aa2adf4aa0a09cb07",
      "5bce3bc823bf4da3af54f54ba75f6aa5",
      "187b78e40a0f40fcaa36d7e77c060432",
      "65b8595f967747e8ba6607da101e9af6",
      "7f771510206e477d9f8aee491a2946f3",
      "a1e1e2bdf12e4f799d30d7b16fb83fd4",
      "76a7ed728b5546a180bf43b5d00289af",
      "40de6f861ecd4cd89f1c371b3a61c2af",
      "06b80423e8d7492eaf57f25d69121d15",
      "cd6759f9d1594be2b2539ac8288a3121",
      "5ece41704e564be3a33e43f0d9784ecf",
      "db480304a3f14636aecd05cb5e84a47c"
     ]
    },
    "id": "hjLBKY6q4PJQ",
    "outputId": "381786d5-c0ed-49af-d2a3-8050d2fcad7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'yelp_review_full'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde84cdc05f94189bc29f389306abac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b92d008b4604d0f8afc54a2acdcf970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/299M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493fb77e42154b97b06a39281ffbce02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/23.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff00c7b2d474cea993f2ec74ee1b37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738b91cfe4ab446d908cd144248e7828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 5 classes in dataset 'yelp_review_full'\n",
      "Training BPE tokenizer...\n",
      "Tokenizer vocabulary size: 30000\n",
      "Creating memory-efficient datasets...\n",
      "Training samples: 650436\n",
      "Test samples: 50000\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caceb86ca81c42aba7949ad737bd8e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4494c1ed96ef4ae3b737c35788fdec36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Time: 662.36s\n",
      "Train Loss: 0.7732 | Train Acc: 0.6677\n",
      "Val Loss: 0.8870 | Val Acc: 0.6196\n",
      "Checkpoint saved at epoch 1\n",
      "New best model saved with accuracy: 0.6196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bce3bc823bf4da3af54f54ba75f6aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## Transformer Text Classifier with Hugging Face BPE Subtokenizer\n",
    "# # Import required libraries\n",
    "\n",
    "# # Cell 1: Install required packages if not already installed\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import pickle\n",
    "# import math\n",
    "# from datasets import concatenate_datasets\n",
    "# from datasets import load_dataset\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# from tokenizers.processors import BertProcessing\n",
    "# import os\n",
    "# import gc\n",
    "# import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "# import time\n",
    "\n",
    "# # Cell 2: Initialize and train BPE tokenizer\n",
    "# VOCAB_SIZE = 30000\n",
    "# MIN_FREQUENCY = 2\n",
    "# SPECIAL_TOKENS = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
    "\n",
    "# # Train a Byte-Level BPE tokenizer on the training texts\n",
    "# def train_bpe_tokenizer(text_iterable, vocab_size=VOCAB_SIZE, min_frequency=MIN_FREQUENCY):\n",
    "#     # Use a small sample for training the tokenizer to save memory\n",
    "#     if isinstance(text_iterable, list):\n",
    "#         sample_size = min(30000, len(text_iterable))\n",
    "#         import random\n",
    "#         random.seed(42)\n",
    "#         sample_texts = random.sample(text_iterable, sample_size)\n",
    "#     else:\n",
    "#         # For dataset iterables\n",
    "#         sample_texts = []\n",
    "#         for i, text in enumerate(text_iterable):\n",
    "#             if i >= 30000:\n",
    "#                 break\n",
    "#             sample_texts.append(text)\n",
    "\n",
    "#     tokenizer = ByteLevelBPETokenizer(\n",
    "#         lowercase=True,\n",
    "#         add_prefix_space=True\n",
    "#     )\n",
    "#     tokenizer.train_from_iterator(\n",
    "#         sample_texts,\n",
    "#         vocab_size=vocab_size,\n",
    "#         min_frequency=min_frequency,\n",
    "#         special_tokens=SPECIAL_TOKENS\n",
    "#     )\n",
    "#     tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#         (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "#         (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\"))\n",
    "#     )\n",
    "#     tokenizer.enable_truncation(max_length=256)\n",
    "#     tokenizer.enable_padding(\n",
    "#         length=256,\n",
    "#         pad_id=tokenizer.token_to_id(\"[PAD]\"),\n",
    "#         pad_token=\"[PAD]\"\n",
    "#     )\n",
    "#     return tokenizer\n",
    "\n",
    "# # Cell 3: Transformer model architecture (unchanged)\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=512):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# class TransformerInputLayer(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, max_len=256, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "#         self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, input_ids):\n",
    "#         x = self.token_embedding(input_ids)\n",
    "#         x = self.positional_encoding(x)\n",
    "#         return self.dropout(x)\n",
    "\n",
    "# class MultiHeadSelfAttention(nn.Module):\n",
    "#     def __init__(self, d_model, num_heads):     # apple [128] ---> Wq , Wk , Wv  --> Q,K,V  = Q.K = attention(0.78) --> [128] -->king\n",
    "#         super().__init__()\n",
    "#         assert d_model % num_heads == 0\n",
    "#         self.d_k = d_model // num_heads\n",
    "#         self.num_heads = num_heads\n",
    "#         self.q_linear = nn.Linear(d_model, d_model)\n",
    "#         self.k_linear = nn.Linear(d_model, d_model)\n",
    "#         self.v_linear = nn.Linear(d_model, d_model)\n",
    "#         self.out_linear = nn.Linear(d_model, d_model)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         B, T, D = x.size()\n",
    "#         Q = self.q_linear(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "#         K = self.k_linear(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "#         V = self.v_linear(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "#         scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "#         if mask is not None:\n",
    "#             scores = scores.masked_fill(mask == 0, -1e9)\n",
    "#         attn = torch.softmax(scores, dim=-1)\n",
    "#         out = torch.matmul(self.dropout(attn), V)\n",
    "#         out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "#         return self.out_linear(out)\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, d_model, d_ff):\n",
    "#         super().__init__()\n",
    "#         self.linear1 = nn.Linear(d_model, d_ff)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.linear2 = nn.Linear(d_ff, d_ff//2)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.linear3 = nn.Linear(d_ff//2, d_model)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.linear3(self.dropout(self.relu2(self.linear2(self.relu1(self.linear1(x))))))  #--2 layer\n",
    "# class TransformerEncoderBlock(nn.Module):\n",
    "#     def __init__(self, d_model, num_heads, d_ff):\n",
    "#         super().__init__()\n",
    "#         self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.ff = FeedForward(d_model, d_ff)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         attn_out = self.attn(x, mask)\n",
    "#         x = self.norm1(x + self.dropout(attn_out))\n",
    "#         ff_out = self.ff(x)\n",
    "#         return self.norm2(x + self.dropout(ff_out))\n",
    "\n",
    "# class TransformerEncoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout):\n",
    "#         super().__init__()\n",
    "#         self.input_layer = TransformerInputLayer(vocab_size, d_model, max_len, dropout)\n",
    "#         self.layers = nn.ModuleList([\n",
    "#             TransformerEncoderBlock(d_model, num_heads, d_ff)\n",
    "#             for _ in range(num_layers)\n",
    "#         ])\n",
    "\n",
    "#     def forward(self, input_ids, mask=None):\n",
    "#         x = self.input_layer(input_ids)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, mask)\n",
    "#         return x\n",
    "\n",
    "# class TransformerClassifier(nn.Module):\n",
    "#     def __init__(self, vocab_size, num_classes, d_model, num_heads, d_ff, num_layers, max_len, dropout):\n",
    "#         super().__init__()\n",
    "#         self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout)\n",
    "#         self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "#     def forward(self, input_ids, mask=None):\n",
    "#         x = self.encoder(input_ids, mask)\n",
    "#         cls_out = x[:, 0, :]\n",
    "#         return self.classifier(cls_out)\n",
    "\n",
    "# # Cell 4: Memory-efficient dataset class\n",
    "# class TokenizedDataset(Dataset):\n",
    "#     def __init__(self, dataset, tokenizer, max_len=256, text_field='text', label_field='label'):\n",
    "#         self.dataset = dataset\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = max_len\n",
    "#         self.text_field = text_field\n",
    "#         self.label_field = label_field\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.dataset[idx]\n",
    "#         text = item[self.text_field]\n",
    "#         label = item[self.label_field]\n",
    "\n",
    "#         encoding = self.tokenizer.encode(text)\n",
    "#         input_ids = torch.tensor(encoding.ids, dtype=torch.long)\n",
    "\n",
    "#         return input_ids, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# # Cell 5: Hyperparameters and data loading\n",
    "# D_MODEL = 128  # 512\n",
    "# NUM_HEADS = 4  # 8\n",
    "# D_FF = 512     # 2048\n",
    "# NUM_LAYERS = 2 # 4\n",
    "# MAX_LEN = 256\n",
    "# DROPOUT = 0.1\n",
    "# BATCH_SIZE = 64\n",
    "# LR = 3e-4\n",
    "# EPOCHS = 30\n",
    "# DATASET_NAME = 'yelp_review_full'\n",
    "\n",
    "# # Cell 6: Load and process data\n",
    "# def load_and_process_data(dataset_name=DATASET_NAME):\n",
    "#     print(f\"Loading dataset '{dataset_name}'...\")\n",
    "#     ds = load_dataset(dataset_name)\n",
    "#     train = concatenate_datasets([ds['train'],custom_train])\n",
    "#     test = ds['test']\n",
    "\n",
    "#     unique_labels = set(train['label'])\n",
    "#     num_classes = len(unique_labels)\n",
    "#     print(f\"Detected {num_classes} classes in dataset '{dataset_name}'\")\n",
    "\n",
    "#     print(\"Training BPE tokenizer...\")\n",
    "#     tokenizer = train_bpe_tokenizer(train['text'], vocab_size=VOCAB_SIZE)\n",
    "#     print(f\"Tokenizer vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "#     # Create memory-efficient datasets\n",
    "#     print(\"Creating memory-efficient datasets...\")\n",
    "#     train_dataset = TokenizedDataset(train, tokenizer)\n",
    "#     test_dataset = TokenizedDataset(test, tokenizer)\n",
    "\n",
    "#     print(f\"Training samples: {len(train_dataset)}\")\n",
    "#     print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "#     return tokenizer, train_dataset, test_dataset, num_classes\n",
    "\n",
    "# # Initialize model, loss, and optimizer\n",
    "# def init_model(vocab_size, num_classes):\n",
    "#     model = TransformerClassifier(\n",
    "#         vocab_size=vocab_size,\n",
    "#         num_classes=num_classes,\n",
    "#         d_model=D_MODEL,\n",
    "#         num_heads=NUM_HEADS,\n",
    "#         d_ff=D_FF,\n",
    "#         num_layers=NUM_LAYERS,\n",
    "#         max_len=MAX_LEN,\n",
    "#         dropout=DROPOUT\n",
    "#     )\n",
    "\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Using device: {device}\")\n",
    "#     model.to(device)\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "#     return model, criterion, optimizer, device\n",
    "\n",
    "# # Cell 7: Checkpoint functions\n",
    "# def setup_checkpointing():\n",
    "#     if os.path.exists('/content/drive/MyDrive/model_checkpoints2'):\n",
    "#         CHECKPOINT_DIR = '/content/drive/MyDrive/model_checkpoints2'\n",
    "#     else:\n",
    "#         CHECKPOINT_DIR = './model_checkpoints'\n",
    "\n",
    "#     os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "#     CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'transformer_classifier_checkpoint_best_best.pth')\n",
    "#     return CHECKPOINT_PATH\n",
    "\n",
    "# def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'loss': loss,\n",
    "#     }, path)\n",
    "#     print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "# def load_checkpoint(model, optimizer, path, device):\n",
    "#     if os.path.exists(path):\n",
    "#         checkpoint = torch.load(path, map_location=device)\n",
    "#         model.load_state_dict(checkpoint)\n",
    "#     return 0\n",
    "\n",
    "\n",
    "# # Cell 8: Training and evaluation functions\n",
    "# def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "#     for inputs, labels in progress_bar:\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(inputs)\n",
    "#         loss = criterion(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         preds = torch.argmax(logits, dim=1)\n",
    "#         correct += (preds == labels).sum().item()\n",
    "#         total += labels.size(0)\n",
    "\n",
    "#         # Update progress bar\n",
    "#         progress_bar.set_postfix({\n",
    "#             \"loss\": f\"{loss.item():.4f}\",\n",
    "#             \"acc\": f\"{correct/total:.4f}\"\n",
    "#         })\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     accuracy = correct / total\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# def evaluate(model, test_loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             logits = model(inputs)\n",
    "#             loss = criterion(logits, labels)\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             preds = torch.argmax(logits, dim=1)\n",
    "#             correct += (preds == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "\n",
    "#     avg_loss = total_loss / len(test_loader)\n",
    "#     accuracy = correct / total\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# # Cell 9: Main training loop\n",
    "# def train_model(model, train_dataset, test_dataset, criterion, optimizer, device, checkpoint_path, epochs=EPOCHS):\n",
    "#     # Set up data loaders with appropriate batch size and num_workers\n",
    "#     NWORKERS = 12\n",
    "#     train_loader = DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True,\n",
    "#         num_workers=NWORKERS,\n",
    "#         pin_memory=True\n",
    "#     )\n",
    "\n",
    "#     test_loader = DataLoader(\n",
    "#         test_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=False,\n",
    "#         num_workers=NWORKERS,\n",
    "#         pin_memory=True\n",
    "#     )\n",
    "\n",
    "#     # Try to load checkpoint\n",
    "#     start_epoch = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "\n",
    "#     best_accuracy = 0.0\n",
    "\n",
    "#     for epoch in range(start_epoch, epochs):\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         # Train for one epoch\n",
    "#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "#         # Evaluate\n",
    "#         val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "#         # Report metrics\n",
    "#         epoch_time = time.time() - start_time\n",
    "#         print(f\"Epoch {epoch+1}/{epochs} | Time: {epoch_time:.2f}s\")\n",
    "#         print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "#         print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "#         # Save checkpoint\n",
    "#         save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_path)\n",
    "\n",
    "#         # Save best model\n",
    "#         if val_acc > best_accuracy:\n",
    "#             best_accuracy = val_acc\n",
    "#             best_model_path = checkpoint_path\n",
    "#             torch.save(model.state_dict(), best_model_path)\n",
    "#             print(f\"New best model saved with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "#         # Force garbage collection to free memory\n",
    "#         gc.collect()\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Cell 10: Example inference\n",
    "# def predict_sentiment(text, model, tokenizer, device, num_classes):\n",
    "#     enc = tokenizer.encode(text)\n",
    "#     input_ids = torch.tensor([enc.ids]).to(device)\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(input_ids)\n",
    "#         probs = torch.softmax(logits, dim=1)\n",
    "#         pred = torch.argmax(probs, dim=1).item()\n",
    "#         # Map score to sentiment (for Yelp dataset)\n",
    "#         if num_classes == 5:  # Yelp dataset\n",
    "#             sentiment_map = {\n",
    "#                 0: \"Very Negative (1 star)\",\n",
    "#                 1: \"Negative (2 stars)\",\n",
    "#                 2: \"Neutral (3 stars)\",\n",
    "#                 3: \"Positive (4 stars)\",\n",
    "#                 4: \"Very Positive (5 stars)\"\n",
    "#             }\n",
    "#             return sentiment_map.get(pred, f\"Class {pred}\"), probs[0][pred].item()\n",
    "#         else:\n",
    "#             return pred, probs[0][pred].item()\n",
    "\n",
    "# # Cell 11: Save model artifacts\n",
    "# def save_artifacts(model, tokenizer, device):\n",
    "#     # Save model state\n",
    "#     torch.save(model.state_dict(), \"transformer_classifier.pth\")\n",
    "\n",
    "#     # Save tokenizer\n",
    "#     with open(\"tokenizer.json\", \"w\") as f:\n",
    "#         f.write(tokenizer.to_str())\n",
    "\n",
    "#     print(\"Model and tokenizer saved successfully.\")\n",
    "\n",
    "# # Cell 12: Main execution\n",
    "# def main():\n",
    "#     # Load and process data\n",
    "#     tokenizer, train_dataset, test_dataset, num_classes = load_and_process_data()\n",
    "\n",
    "#     # Initialize model and related components\n",
    "#     model, criterion, optimizer, device = init_model(tokenizer.get_vocab_size(), num_classes)\n",
    "\n",
    "#     # Setup checkpointing\n",
    "#     checkpoint_path = setup_checkpointing()\n",
    "\n",
    "#     with open(\"tokenizer.json\", \"w\") as f:\n",
    "#         f.write(tokenizer.to_str())\n",
    "#     # Train model\n",
    "#     model = train_model(model, train_dataset, test_dataset, criterion, optimizer, device, checkpoint_path)\n",
    "\n",
    "#     # Save artifacts\n",
    "#     save_artifacts(model, tokenizer, device)\n",
    "\n",
    "#     # Example inference\n",
    "#     sample_text = \"The food was delicious and the service was excellent!\"\n",
    "#     sentiment, confidence = predict_sentiment(sample_text, model, tokenizer, device, num_classes)\n",
    "#     print(f\"Sample text: '{sample_text}'\")\n",
    "#     print(f\"Predicted sentiment: {sentiment} (confidence: {confidence:.4f})\")\n",
    "\n",
    "# main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
